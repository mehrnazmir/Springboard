{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capstone Project 1: New York City Taxi Fare Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In big cities such as New York City, a huge number of taxi rides is taken per day. \n",
    "As the popularity of app-based vehicle hiring services grows, accurate prediction of taxi fare is essential for \n",
    "enhancing customersâ€™ satisfaction, since it is given as upfront data to the customers. \n",
    "There are many factors that should be considered such as the pickup time, pickup or dropoff locations, etc. in predicting taxi fare. \n",
    "Providing accurate taxi fare at a specific time enables both drivers and customers to decide whether to select the rides or not. \n",
    "The goal of this project is developing a Machine Learning (ML) based model to predict the fare amount for a taxi ride in New York City while some data such as the pickup and dropoff locations are given. \n",
    "\n",
    "Predicting accurate taxi fares yields better results for taxi cab and ridesharing companies such as Uber, Lyft, etc. \n",
    "Also, this project can be used in traffic congestion prediction and autonomous vehicle research to develop accurate traffic models and choose the fastest and less congested routes. \n",
    "\n",
    "The data from a Kaggle competition is used for this project\n",
    "(https://www.kaggle.com/c/new-york-city-taxi-fare-prediction/overview).\n",
    "\n",
    "The dataset for this project includes the features explained below:\n",
    "\n",
    "   * pickup_datetime - timestamp value indicating when the taxi ride started.\n",
    "    \n",
    "   * pickup_longitude - float for longitude coordinate of where the taxi ride started.\n",
    "    \n",
    "   * pickup_latitude - float for latitude coordinate of where the taxi ride started.\n",
    "    \n",
    "   * dropoff_longitude - float for longitude coordinate of where the taxi ride ended.\n",
    "    \n",
    "   * dropoff_latitude - float for latitude coordinate of where the taxi ride ended.\n",
    "    \n",
    "   * passenger_count -integer indicating the number of passengers in the taxi ride.\n",
    "\n",
    "During the modeling phase of the project, these features can be extended. \n",
    "\n",
    "Target: \n",
    "\n",
    "   * fare_amount - dollar amount of the cost of the taxi ride. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read limited number of rows from dataset due to low memory. For this project, 6 million rows are read from 55 million available rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File b'/Users/mehrnaz/Desktop/SpringBoard/Assignment/Capstone_Project_1/Data_wrangling/train.csv' does not exist: b'/Users/mehrnaz/Desktop/SpringBoard/Assignment/Capstone_Project_1/Data_wrangling/train.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-c78ffee15815>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/Users/mehrnaz/Desktop/SpringBoard/Assignment/Capstone_Project_1/Data_wrangling/train.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m6000000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    683\u001b[0m         )\n\u001b[1;32m    684\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 685\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    686\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1135\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1136\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1915\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1917\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1918\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] File b'/Users/mehrnaz/Desktop/SpringBoard/Assignment/Capstone_Project_1/Data_wrangling/train.csv' does not exist: b'/Users/mehrnaz/Desktop/SpringBoard/Assignment/Capstone_Project_1/Data_wrangling/train.csv'"
     ]
    }
   ],
   "source": [
    "train_file = pd.read_csv('/Users/mehrnaz/Desktop/SpringBoard/Assignment/Capstone_Project_1/Data_wrangling/train.csv', nrows = 6000000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part1: Data Wrangling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First try to see:\n",
    "   * How train_file dataframe looks like. \n",
    "   * Shape of train_file. \n",
    "   * Statistics of the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check data type of each column\n",
    "train_file.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let see how dataframe looks like \n",
    "train_file.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check statistics of the features\n",
    "train_file.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check the number of rows and columns of 'train_file' dataframe\n",
    "train_file.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the above information, following steps should be performed:\n",
    "\n",
    "* Check if there is any NAN and drop them\n",
    "\n",
    "* Check the target column:\n",
    "\n",
    "    * E.g. negative fare_amount does not make sense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check how manay NANs exsit in dataset\n",
    "train_file.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove NAN from the file \n",
    "train_file = train_file.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check the number of rows and columns after removing NAN\n",
    "train_file.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check if there is negative value for fare_amount and how many\n",
    "Counter(train_file['fare_amount'] <= 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove rows with negative and zero values for 'fare_amount'\n",
    "train_file = train_file.drop(train_file[train_file['fare_amount'] <= 0].index, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check the number of rows and columns after removing 'fare_amount' with negative value\n",
    "train_file.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot histogram for 'fare_amount'\n",
    "train_file.hist(column='fare_amount', log=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the histogram for 'fare_amount', the price range between (0,200) dollar makes sense.\n",
    "\n",
    "Let see how many 'fare_amount' greater than 200 dollar exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check if there is 'fare_amount' greater than $200 \n",
    "Counter(train_file['fare_amount'] > 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Does not make sense to have 'fare_amount' greater than $200, so consider them as outlier and remove them\n",
    "train_file = train_file.drop(train_file[train_file['fare_amount'] > 200].index, axis=0)\n",
    "train_file.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next analyzing the problem features.\n",
    "\n",
    "In order to analyze latitude and longtitude columns, coordinates of NYC should be considered as boundries.\n",
    "\n",
    "Googled to find the latitude and longtitude range for NYC:\n",
    "* The NYC's latitude is in the range of (40, 42) \n",
    "\n",
    "* The NYC longtitude is in the range of (-76, -71) \n",
    "\n",
    "I have considered a slightly wider range for latitude and longtitude to be more inclusive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check to see if there is an outlier for pickup_longitude\n",
    "train_file['pickup_longitude'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check to see if there is an outlier for pickup_latitude\n",
    "train_file['pickup_latitude'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check to see if there is an outlier for pickup_longitude\n",
    "train_file['pickup_longitude'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check to see if there is an outlier for pickup_latitude\n",
    "train_file['pickup_latitude'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check to see if there is an outlier for dropoff_longitude\n",
    "train_file['dropoff_longitude'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check to see if there is an outlier for dropoff_longitude\n",
    "train_file['dropoff_longitude'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check to see if there is an outlier for dropoff_latitude\n",
    "train_file['dropoff_latitude'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check to see if there is an outlier for dropoff_latitude\n",
    "train_file['dropoff_latitude'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#'pickup_latitude' should be in the range of (40, 42)\n",
    "train_file = train_file[train_file['pickup_latitude'].between(40, 42)]\n",
    "train_file.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#'pickup_longitude' should be in the range of (-76, -71)\n",
    "train_file = train_file[train_file['pickup_longitude'].between(-76,-71)]\n",
    "train_file.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#'dropoff_latitude' should be in the range of (40, 42)\n",
    "train_file = train_file[train_file['dropoff_latitude'].between(40, 42)]\n",
    "train_file.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#'dropoff_longitude' should be in the range of (-76, -71)\n",
    "train_file = train_file[train_file['dropoff_longitude'].between(-76,-71)]\n",
    "train_file.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The other column that should be cleaned up is the 'passenger_count'.\n",
    "\n",
    "Let's find if there is an outlier for this feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file.hist(column='passenger_count', log=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check if there is outlier for passenger_count\n",
    "train_file['passenger_count'].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maximum number of passengers is 208 that does not make sense for the number of seats on a taxi cab.\n",
    "The maxminum allowed passengers for an SUV or a Van is 6. So, 6 is considered as an upperbound for the number of passengers in each ride."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Does not make sense to have 'passenger_count' greater than 6 or less than 1, so consider them as bounds and remove the data out of bounds.\n",
    "train_file = train_file[train_file['passenger_count'].between(1, 6)]\n",
    "train_file.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next step, new features will be created based on the available data to see whether these features affect the fare_amount or not.\n",
    "    * Distance between pickup and dropoff location should be calculated.\n",
    "    * The date and time of pickup. \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Haversine formula is employed to calculate the distance between pickup and dropoff locations based on longitude and latitude. \n",
    "\n",
    "The Haversine formula is (https://en.wikipedia.org/wiki/Haversine_formula):\n",
    "\n",
    "distance = 2 * r * arcsin(sqrt(sin((latitude2 - latitude1) / 2.0)^2 + cos(latitude1) * cos(latitude2) * sin((longitude2 - longitude1) / 2.0)^2))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate the distance based on Haversine formula\n",
    "def distance(lat1, lat2, lon1, lon2):\n",
    "    # radians which converts from degrees to radians.   \n",
    "    lat1 = np.radians(lat1) \n",
    "    lat2 = np.radians(lat2)\n",
    "    lon1 = np.radians(lon1) \n",
    "    lon2 = np.radians(lon2) \n",
    "        \n",
    "    # Haversine formulation  \n",
    "    dlon = lon2 - lon1  \n",
    "    dlat = lat2 - lat1\n",
    "    a = np.sin(dlat / 2.0)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2.0)**2\n",
    "    \n",
    "    c = 2 * np.arcsin(np.sqrt(a))\n",
    "    r = 6371\n",
    "       \n",
    "    # calculate the result \n",
    "    dis = c * r\n",
    "    return dis\n",
    "\n",
    "def creat_new_cloumn(df):\n",
    "    data = [df]\n",
    "    for row in data:\n",
    "        row['distance'] = distance(row['pickup_latitude'], row['dropoff_latitude'], row['pickup_longitude'], row['dropoff_longitude'])\n",
    "    return row['distance'] \n",
    "creat_new_cloumn(train_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check if there are ouliers for distance\n",
    "train_file['distance'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check if there are ouliers for distance\n",
    "train_file['distance'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file.hist(column='distance', log=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Does not make sense to have 'distance' greater than 150km, so consider them as outlier and remove them\n",
    "train_file = train_file.drop(train_file[train_file['distance'] > 150].index, axis=0)\n",
    "train_file.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zero 'distance' does not make sense, so consider them as outlier and remove them\n",
    "train_file = train_file.drop(train_file[train_file['distance'] == 0].index, axis=0)\n",
    "train_file.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fare_amount usually changes based on the days of the week. To explore how price change and affect the 'fare_amount', the type of the key column should be converted to datetime type to create new year, month, day, dayofweek, and hour columns. Since the 'key' and 'pickup_datetime' columns are the same, the 'key' column should be removed to get rid of duplicated data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert the 'key' column to datetime \n",
    "train_file['key'] = pd.to_datetime(train_file['key'])\n",
    "train_file.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add 'year' column to dataframe\n",
    "train_file['year'] = train_file['key'].dt.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add 'year' column to dataframe\n",
    "train_file['month'] = train_file['key'].dt.month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add 'year' column to dataframe\n",
    "train_file['dayofweek'] = train_file['key'].dt.dayofweek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add 'year' column to dataframe\n",
    "train_file['day'] = train_file['key'].dt.day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add 'year' column to dataframe\n",
    "train_file['hour'] = train_file['key'].dt.hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make sure only data for recent years 2005 to 2020 are considered\n",
    "train_file = train_file[train_file['year'].between(2005, 2020)]\n",
    "train_file.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove 'key' column to remove duplicated data\n",
    "train_file = train_file.drop('key', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Story Telling\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Draw the fare_amount distribution\n",
    "plt.figure(figsize = (10, 6))\n",
    "sns.distplot(train_file['fare_amount'])\n",
    "plt.title('Fare_Amount Distribution')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fare amount distribution shows that most of the rides are under $25. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see where pickup and dropoff coordinates are located on the map. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file.plot(kind='scatter', x='pickup_longitude', y='pickup_latitude', color='r')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file.plot(kind='scatter', x='dropoff_longitude', y='dropoff_latitude', color='b')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown in these plots, the concentration of coordinates are between (-74.1, -73.8) and (40.6, 40.8) approximatly.\n",
    "The coordinates are related to Manhattan neighberhood, since this area is considered as the crowded part of NYC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how number of passenger has impacted fare amount."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Average fare_amount by passenger_count.\n",
    "train_file.groupby('passenger_count')['fare_amount'].mean().plot.bar()\n",
    "plt.title('fare_amount per passenger_count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x=train_file['passenger_count'], y=train_file['fare_amount'], s=1.5)\n",
    "plt.xlabel('passenger_count')\n",
    "plt.ylabel('fare_amount')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above figures for 'fare_amount' based on 'passenger_count' represent that the 'fare_amount' includes a base amount that increases slightly by the number of passengers in this case for single to five passengers. Then, the 'fare_amount' significantly increased for six passengers because another type of car is required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To better explore the data, three new columns should be added to calculate fare_amount normalized based on distance and passenger_count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file['fare_amount/distance'] = train_file.apply(lambda row: row.fare_amount / row.distance, axis = 1)\n",
    "train_file['fare_amount/passenger_count'] = train_file.apply(lambda row: row.fare_amount / row.passenger_count, axis = 1)\n",
    "train_file['base_fare'] = train_file.apply(lambda row: row.fare_amount / row.passenger_count / row.distance, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to see how days of the week affect the 'fare_amount', other features should be considered constant.\n",
    "To show how weekdays can affect the 'fare_amount', data corresponding to year 2015 and 1.0 < distance < 3.5 are studied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filter train_file data frame to show the \n",
    "filter1_train_file = train_file[(train_file['year'] == 2015) & (train_file['distance'].between(10,30)) & (train_file['passenger_count'].between(2,4))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter1_train_file.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " filter1_train_file.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The day of the week with Monday=0, Sunday=6\n",
    "filter1_train_file.groupby('dayofweek')['fare_amount'].mean().plot()\n",
    "plt.title('average fare_amount base on dayofweek')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results shows that Monday has the highest 'fare_amount'.\n",
    "Also, 'fare_amount' is lowest during the weekend."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Showing how fare amount per number of passengers is changed over the years can give us a perspective of how it may change in the coming years. Therefore, trend of fare amount per number of passenger is plotted as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Show the trend of average 'fare_amount' per person during years\n",
    "train_file.groupby('year')['fare_amount/passenger_count'].mean().plot()\n",
    "plt.title('fare_amount/passenger_count Trend')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the figure show that fare_amount is increased over the years.\n",
    "However, increas is slowing down since 2013."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The city grows over the years and it may have huge impact on fare amount.\n",
    "Checking average distance that passengers commute over the years can explain the changes in fare amount. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#See trend of distance during the years\n",
    "train_file.groupby('year')['distance'].mean().plot()\n",
    "plt.title('trend of distance during the years')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distance that passengers commute has increased during the years. It can cause the raise in the fare_amount.\n",
    "The biggest jump is from 2010 to 2011 but it slightly decreased in 2015."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how many passengers majority of rides has had:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#See passenger_count vs. distance\n",
    "train_file.groupby('passenger_count')['distance'].count().plot()\n",
    "plt.title('passenger_count vs. ride count' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot shows that majority of the rides has single passenger. The rides with two passengers is the second most. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
